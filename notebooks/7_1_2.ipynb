{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "openjdk-8-jre-headless is already the newest version (8u275-b01-0ubuntu1~20.04).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 9 not upgraded.\n",
      "Requirement already satisfied: pyspark in /home/ubuntu/env/lib/python3.8/site-packages (3.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9 in /home/ubuntu/env/lib/python3.8/site-packages (from pyspark) (0.10.9)\n"
     ]
    }
   ],
   "source": [
    "!sudo apt install -y openjdk-8-jre-headless\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.64.3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f494c2abca0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23583 20200101 0005 20191231 1505      3 -158.61   59.28   -17.5     0.0     30 0   -21.8 C 0    60 0 -99.000 -9999.0  1015 0   1.59 0',\n",
       " '23583 20200101 0010 20191231 1510      3 -158.61   59.28   -17.5     0.0     35 0   -22.1 C 0    60 0 -99.000 -9999.0  1015 0   1.35 0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.sparkContext.textFile('/home/ubuntu/raw/*')\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188256"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(timestamp=datetime.datetime(2020, 1, 1, 0, 5, tzinfo=datetime.timezone.utc), wbanno='23583', temperature=-17.5),\n",
       " Row(timestamp=datetime.datetime(2020, 1, 1, 0, 10, tzinfo=datetime.timezone.utc), wbanno='23583', temperature=-17.5)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def parse_line(line):\n",
    "    f = line.split()\n",
    "    wbanno = f[0]\n",
    "    dt = datetime.strptime(f[1] + f[2], '%Y%m%d%H%M')\n",
    "    dt = dt.replace(tzinfo=timezone.utc)\n",
    "    temperature = None if f[8] == '-9999.0' else float(f[8])\n",
    "    return Row(timestamp=dt, wbanno=wbanno, temperature=temperature)\n",
    "\n",
    "rows = rdd.map(parse_line)\n",
    "rows.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.session.timeZone\", 'UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp: timestamp, wbanno: string, temperature: double]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = rdd.map(parse_line).toDF()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-----------+\n",
      "|          timestamp|wbanno|temperature|\n",
      "+-------------------+------+-----------+\n",
      "|2020-01-01 00:05:00| 23583|      -17.5|\n",
      "|2020-01-01 00:10:00| 23583|      -17.5|\n",
      "+-------------------+------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|            wbanno|       temperature|\n",
      "+-------+------------------+------------------+\n",
      "|  count|            188256|            188052|\n",
      "|   mean|           25119.5|1.5413896156381783|\n",
      "| stddev|1536.5040808954864| 11.66942920389126|\n",
      "|    min|             23583|             -32.0|\n",
      "|    max|             26656|              24.8|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('uscrn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-----+-------------------+-----+\n",
      "|wbanno|      timestamp_min|t_min|      timestamp_max|t_max|\n",
      "+------+-------------------+-----+-------------------+-----+\n",
      "| 23583|2020-02-01 16:15:00|-32.0|2020-08-17 00:20:00| 24.8|\n",
      "| 26656|2020-02-09 15:15:00|-30.8|2020-05-30 23:05:00| 23.3|\n",
      "+------+-------------------+-----+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT\n",
    "  wbanno,\n",
    "  min_by(timestamp, temperature) timestamp_min,\n",
    "  min(temperature) t_min,\n",
    "  max_by(timestamp, temperature) timestamp_max,\n",
    "  max(temperature) t_max\n",
    "FROM\n",
    "  uscrn\n",
    "GROUP by\n",
    "  1\n",
    "'''\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.save('./uscrn-parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\n",
      "part-00000-5cb3cd43-c795-49ce-9d46-ead6520135d2-c000.snappy.parquet\n",
      "part-00001-5cb3cd43-c795-49ce-9d46-ead6520135d2-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls ./uscrn-parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|wbanno|  avg(temperature)|\n",
      "+------+------------------+\n",
      "| 23583|2.5634793606257302|\n",
      "| 26656|0.5178639846742709|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load('./uscrn-parquet')\n",
    "df.groupBy('wbanno').avg('temperature').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>avg(temperature)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42528</th>\n",
       "      <td>2020-07-17 00:50:00</td>\n",
       "      <td>22.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7317</th>\n",
       "      <td>2020-08-17 02:55:00</td>\n",
       "      <td>22.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp  avg(temperature)\n",
       "42528 2020-07-17 00:50:00              22.9\n",
       "7317  2020-08-17 02:55:00              22.7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Dataframe\n",
    "\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", 'UTC')\n",
    "\n",
    "df = spark.read.load('./uscrn-parquet')\n",
    "df1 = df.groupBy('timestamp').avg().toPandas()\n",
    "df1.sort_values(by='avg(temperature)', ascending=False).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /home/ubuntu/env/lib/python3.8/site-packages (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.14 in /home/ubuntu/env/lib/python3.8/site-packages (from pyarrow) (1.19.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-07-17 00:50:00</th>\n",
       "      <td>22.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-17 02:55:00</th>\n",
       "      <td>22.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     temperature\n",
       "timestamp                       \n",
       "2020-07-17 00:50:00         22.9\n",
       "2020-08-17 02:55:00         22.7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('./uscrn-parquet')\n",
    "\n",
    "df1 = df.groupby('timestamp').mean()\n",
    "df1.sort_values(by='temperature', ascending=False).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52414"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", 'UTC')\n",
    "\n",
    "df = spark.read.load('./uscrn-parquet')\n",
    "df1 = df.where(\"timestamp >= '2020-01-01' AND timestamp < '2020-04-01'\")\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.coalesce(1).write.save('./export', format='csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS  part-00000-b31b685e-f733-4007-b11c-b8a9ea6b2ee8-c000.csv\n"
     ]
    }
   ],
   "source": [
    "!ls ./export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./export/*.csv ~/Home/Desktop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
