{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wdpressplus-bigdata/wdpressplus-bigdata/blob/main/notebooks/7-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWVhxY9ktUQm"
   },
   "outputs": [],
   "source": [
    "# p.273 List 7.2\n",
    "import pathlib\n",
    "import requests\n",
    "\n",
    "def download_file(filename):\n",
    "    prefix = 'https://github.com/wdpressplus-bigdata/uscrn/raw/main'\n",
    "    # prefix = 'https://www1.ncdc.noaa.gov/pub/data/uscrn/products/subhourly01'\n",
    "    r = requests.get(f\"{prefix}/2020/{filename}\")\n",
    "    r.raise_for_status()\n",
    "    path = pathlib.Path('./raw')\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path / filename, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    print(f\"Saved {path / filename}\")\n",
    "\n",
    "FILES = [\n",
    "    'CRNS0101-05-2020-AK_Aleknagik_1_NNE.txt',\n",
    "    'CRNS0101-05-2020-AK_Bethel_87_WNW.txt',\n",
    "]\n",
    "for filename in FILES:\n",
    "    download_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zY0KX2uUtUQm"
   },
   "outputs": [],
   "source": [
    "# p.274\n",
    "!pip install pandas    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKJ8DJ72tUQm"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def read_tables():\n",
    "    for path in glob.glob('./raw/*.txt'):\n",
    "        yield pd.read_table(path, delimiter='\\s+', header=None, dtype='str')\n",
    "\n",
    "df = pd.concat(read_tables())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBoqIzvrtUQm"
   },
   "outputs": [],
   "source": [
    "# p.275\n",
    "df1 = df[[0, 1, 2, 8]]\n",
    "df1.columns = ['wbanno', 'utc_date', 'utc_time', 'temperature']\n",
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWWv-OJ5tUQn"
   },
   "outputs": [],
   "source": [
    "df2 = df1.copy()\n",
    "df2.index = pd.to_datetime(df2['utc_date'] + df2['utc_time'])\n",
    "df2.drop(columns=['utc_date', 'utc_time'], inplace=True)\n",
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xupd_r5dtUQn"
   },
   "outputs": [],
   "source": [
    "# p.276\n",
    "df2.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Frk4kYFQtUQn"
   },
   "outputs": [],
   "source": [
    "df2['temperature'] = df2['temperature'].astype('float')\n",
    "df2.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0D6LqBbtUQn"
   },
   "outputs": [],
   "source": [
    "# p.277\n",
    "df3 = df2.copy()\n",
    "df3.loc[df3['temperature'] == -9999.0, 'temperature'] = None\n",
    "df3.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HW9qrrVtUQn"
   },
   "outputs": [],
   "source": [
    "# p.278\n",
    "!pip install pyspark==3.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXEBQWh3zIXU"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUbKGtdOz3K1"
   },
   "outputs": [],
   "source": [
    "# p.279\n",
    "rdd = spark.sparkContext.textFile('./raw/*')\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_yNuuT2z7Cl"
   },
   "outputs": [],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPhHTyQ-09Wn"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def parse_line(line):\n",
    "    f = line.split()\n",
    "    wbanno = f[0]\n",
    "    dt = datetime.strptime(f[1] + f[2], '%Y%m%d%H%M')\n",
    "    dt = dt.replace(tzinfo=timezone.utc)\n",
    "    temperature = None if f[8] == '-9999.0' else float(f[8])\n",
    "    return Row(timestamp=dt, wbanno=wbanno, temperature=temperature)\n",
    "\n",
    "rows = rdd.map(parse_line)\n",
    "rows.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNZPFInj0_dv"
   },
   "outputs": [],
   "source": [
    "# p.280\n",
    "df = rdd.map(parse_line).toDF()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5_iqkRQ1EEn"
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.session.timeZone\", 'UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAKfEX1-1Htn"
   },
   "outputs": [],
   "source": [
    "# p.281\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALBN8_u31SM-"
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('uscrn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LhLXFP_1XK-"
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT\n",
    "  wbanno,\n",
    "  min_by(timestamp, temperature) timestamp_min,\n",
    "  min(temperature) t_min,\n",
    "  max_by(timestamp, temperature) timestamp_max,\n",
    "  max(temperature) t_max\n",
    "FROM\n",
    "  uscrn\n",
    "GROUP by\n",
    "  1\n",
    "'''\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvOyXJ9G1Z0e"
   },
   "outputs": [],
   "source": [
    "# p.283\n",
    "df.write.save('./uscrn-parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9HfNjSW1e7F"
   },
   "outputs": [],
   "source": [
    "!ls ./uscrn-parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OldBX49a1hX2"
   },
   "outputs": [],
   "source": [
    "df = spark.read.load('./uscrn-parquet')\n",
    "df.groupBy('wbanno').avg('temperature').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1hKDIvRr1uUt"
   },
   "outputs": [],
   "source": [
    "# p.284\n",
    "df = spark.read.load('./uscrn-parquet')\n",
    "df1 = df.where(\"timestamp >= '2020-01-01' AND timestamp < '2020-04-01'\")\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSzeZW7j9Yb_"
   },
   "outputs": [],
   "source": [
    "df1.coalesce(1).write.save('./export', format='csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cseQxaw59aZd"
   },
   "outputs": [],
   "source": [
    "!ls ./export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./export/*.csv ~/Home/Desktop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.285 Column\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAenhKff1kS1"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", 'UTC')\n",
    "\n",
    "df = spark.read.load('./uscrn-parquet')\n",
    "df1 = df.groupBy('timestamp').avg().toPandas()\n",
    "df1.sort_values(by='avg(temperature)', ascending=False).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcIkTmtq1rT2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('./uscrn-parquet')\n",
    "\n",
    "df1 = df.groupby('timestamp').mean()\n",
    "df1.sort_values(by='temperature', ascending=False).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "7-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
